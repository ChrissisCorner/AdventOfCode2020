{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gradient_descent.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChrissisCorner/AdventOfCode2020/blob/main/gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyJ_XLlLKAls"
      },
      "source": [
        "Version: 2021.01.25\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRPJSPbQLkzC"
      },
      "source": [
        "# Intelligente Systeme - Übung Lineare Regression\r\n",
        "\r\n",
        "Ziel der linearen Regression ist es, eine lineare Funktion (oder Hypothese) zu finden, welche die Beispieldaten möglichst akkurat beschreibt und mit der sich später wiederum Vorhersagen machen lassen. Dazu wurden in der Vorlesung zwei Verfahren vorgestellt. Beide Verfahren wollen die Fehlerfunktion minimieren und verwenden dazu den Gradienten der Fehlerfunktion. Der Gradient einer Funktion $f(x,y)$ ist definiert als:\r\n",
        "\r\n",
        "$$\\nabla f(x,y) = \\begin{pmatrix}\\frac{\\partial \\mathcal{f}}{\\partial x}\\\\ \\frac{\\partial \\mathcal{f}}{\\partial y}\\end{pmatrix}$$\r\n",
        "\r\n",
        "Mit der geschlossenen Lösung haben wir ein exaktes analytisches Verfahren zur Bestimmung des Minimums betrachtet. Eine numerische Lösung findet der Gradientenabstieg, mit dessen Hilfe wir uns der analytischen Lösung immer weiter annähern. \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTOgbX0dLuVd"
      },
      "source": [
        "# Aufgabe 1 - Gradientenberechnung\n",
        "\n",
        "1. Berechnen Sie die Gradienten der folgenden beiden Funktionen\n",
        "\n",
        "$$f(x, y) = \\frac{1}{x^2+y^2}$$\n",
        " \n",
        "$$f(x, y) = x^2y$$\n",
        "\n",
        "2. Das Gradientenabstiegsverfahren lässt sich nicht nur im Kontext der linearen Regression nutzen, sondern ganz allgemein um Minima von Funktionen zu finden. Schreiben Sie die Update-Gleichungen der beiden Funktionen auf, die Sie nutzen würden, um auf diesem Weg ihre Minima zu bestimmen. Welche Eigenschaft hat ein Gradient und wie wird diese hier genutzt? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK4WU-6qORC9"
      },
      "source": [
        "# Aufgabe 2 - Lineare Regression\n",
        "Die folgende Tabelle gibt den Treibstoffverbrauch $c$ in $\\frac{l}{100 \\text{km}}$ bei gegebener Fahrtgeschwindigkeit $s$ in $\\frac{\\text{km}}{\\text{h}}$ wieder: \n",
        "\n",
        "|$s$|$c$|\n",
        "|--|--|\n",
        "|0|\t0|\n",
        "|30\t|3.5|\n",
        "|50|5|\n",
        "|80|6.8|\n",
        "|100|7.4|\n",
        "|130|8|\n",
        "|180|\t12|\n",
        "\n",
        "Wir wollen eine lineare Funktion finden, die den Zusammenhang beschreibt.\n",
        "\n",
        "1. Schreiben Sie eine geeignete Loss-Funktion $\\mathcal{L}(\\vec{w})$ für $n$ Datenpunkte $(s_i, c_i)$ auf. Benutzen Sie eine lineare Funktion $c(s) = w_1 s + w_0$ als Hypothese. Welche Möglichkeiten gibt es und welche Eigenschaften sind für eine Loss Funktion relevant? Warum werden nicht einfach alle Fehler aufsummiert?\n",
        "\n",
        "2. Leiten Sie für den Gradientenabstieg die Update-Gleichungen für $w_1$ und $w_0$ her. \n",
        "\n",
        "3. Vervollständigen Sie entsprechend der Update-Gleichungen den untenstehenden Code. Probieren Sie auch unterschiedliche Startwerte $w_0$ und $w_1$ aus. Was passiert für zu große Lernraten $\\alpha$, was für zu kleine Lernraten $\\alpha$?\n",
        "\n",
        "4. Bestimmen Sie durch Nullsetzen des Gradienten die optimalen $w_0$ und $w_1$ (geschlossene Lösung) und vergleichen Sie mit der numerisch ermittelten Lösung (Gradientenabstieg).\n",
        "\n",
        "5. *(Zusatz) Auch für die folgende allgemeine Hypothese $$y(x) = \\sum_{i=1}^m w_i f_i(x)$$ kann man die Loss-Funktion aufschreiben und durch Nullsetzen des Gradienten die optimalen Gewichte $w_i$ bestimmen. Versuchen Sie dies.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiWK0etTLhPj"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def update(w1, w0, alpha, s, c):\n",
        "  n = len(s)\n",
        "  dw0 = -(1/n) *np.sum(c-(w1*s-w0))\n",
        "  dw1 = -(1/n) *np.sum(c-(w1*s-w0)*s)\n",
        "\n",
        "  w1 = w1-alpha*dw1\n",
        "  w0 = w0-alpha*dw0\n",
        "\n",
        "  return w1, w0\n",
        "\n",
        "\n",
        " #2.4\n",
        "bestw1 = (np.mean(c*s) - np.mean(c)*np.mean(s))/(np.mean(s**2) - np.mean(s)**2)\n",
        "bestw0 = np.mean(c) - bestw1*np.mean(s)\n",
        "\n",
        "print(f\"Numerische Lösung: c = {bestw1:.2f}s + {bestw0:.2f}\")\n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "s = np.array([0, 30, 50, 80, 100, 130, 180])\n",
        "c = np.array([0, 3.5, 5.0, 6.8, 7.4, 8.0, 12.0])\n",
        "\n",
        "iterations = 100\n",
        "\n",
        "# Startwerte\n",
        "w1 = 2\n",
        "w0 = 2\n",
        "\n",
        "# Lernrate\n",
        "alpha = 0.0001\n",
        "\n",
        "for i in range(iterations):\n",
        "  w1, w0 = update(w1, w0, alpha, s, c)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Numerische Lösung: c = {w1:.2f}s + {w0:.2f}\")\n",
        "\n",
        "plt.figure()\n",
        "plt.xlabel(r\"$s/\\frac{km}{h}$\")\n",
        "plt.ylabel(r\"$c/\\frac{l}{100km}$\")\n",
        "plt.plot(s, c, '.')\n",
        "plt.plot(s, s*w1 + w0)\n",
        "plt.plot(s, s*bestw1 + bestw0)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyslSSr6WONw"
      },
      "source": [
        "# Aufgabe 3 - Visualisierung Gradientenabstiegsverfahren\n",
        "\n",
        "Für die folgende Aufgabe verwenden wir das Doppelmuldenpotential \n",
        "\n",
        "$$V(x) = ax^4 + bx^2 + cx + d$$\n",
        "\n",
        "mit $a = 1$, $b = -3$, $c =1$ und $d = 3.514$. \n",
        "\n",
        "Wir wollen mithilfe des Gradientenabstiegsverfahren das globale Minimum $x_{min}$ dieser Funktion ermitteln. Sie können sich vorstellen, dass $V$ eine Loss-Funktion mit nur einem Gewicht $x$ beschreibt. \n",
        "\n",
        "1. Berechnen Sie die Ableitung und Update-Gleichung für das Gewicht $x$ mit Lernrate $\\alpha$.\n",
        "\n",
        "2. Vervollständigen Sie entsprechend unten stehenden Code.\n",
        "\n",
        "3. Testen Sie die folgenden Kombinationen für Startwert und Lernrate $(x_0, \\alpha)$. \n",
        "$$(x_0, \\alpha) = (-1.75, 0.001)$$\n",
        "$$(x_0, \\alpha) = (-1.75, 0.19)$$\n",
        "$$(x_0, \\alpha) = (-1.75, 0.1)$$\n",
        "$$(x_0, \\alpha) = (-1.75, 0.205)$$\n",
        "\n",
        "4. Wie kann man einen Kompromiss zwischen $(x_0, \\alpha) = (-1.75, 0.001)$ und $(x_0, \\alpha) = (-1.75, 0.19)$ schaffen?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_O7Xu1wlaAu"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def update2(x, a, b, c, d, alpha):\n",
        "  x = ___\n",
        "\n",
        "  return x\n",
        "\n",
        "def V(x, a, b, c, d):\n",
        "  return a*x**4 + b*x**2 + c*x + d\n",
        "\n",
        "a = 1\n",
        "b = -3\n",
        "c = 1\n",
        "d = 3.514\n",
        "\n",
        "x0 = -1.75\n",
        "iterations = 101\n",
        "alphas = np.array([0.001, 0.19, 0.1, 0.205])\n",
        "\n",
        "losses = np.empty(shape=(iterations, len(alphas)))\n",
        "results = np.empty(len(alphas))\n",
        "\n",
        "for j in range(len(alphas)):\n",
        "  x = x0\n",
        "  alpha = alphas[j]\n",
        "  for i in range(iterations):\n",
        "    losses[i, j] = V(x, a, b, c, d)\n",
        "    if i != iterations - 1:\n",
        "      x = update2(x, a, b, c, d, alpha)\n",
        "  results[j] = x\n",
        "\n",
        "for j in range(len(alphas)):\n",
        "  print(100*\"-\")\n",
        "  print(\"Alpha: \", alphas[j])\n",
        "  print(\"xmin: \", results[j])\n",
        "  print(\"Loss: \", V(results[j], a, b, c, d))\n",
        "\n",
        "colors = {\n",
        "    0.001: \"blue\",\n",
        "    0.19: \"red\",\n",
        "    0.1: \"black\",\n",
        "    0.205: \"orange\"\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.title(\"Lernkurven\")\n",
        "plt.xlabel(\"Epoche\")\n",
        "plt.ylabel(\"Loss V\")\n",
        "plt.xlim(0, iterations)\n",
        "\n",
        "for i in range(len(alphas)):\n",
        "  alpha = alphas[i]\n",
        "  plt.plot(range(iterations), losses[:, i], label=str(alpha), color=colors[alpha])\n",
        "\n",
        "plt.legend()\n",
        "plt.ylim(bottom=0)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.title(\"Funktion V und Minima\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"V(x)\")\n",
        "\n",
        "xs = np.linspace(-2, 2, 100)\n",
        "ys = V(xs, a, b, c, d)\n",
        "\n",
        "plt.plot(xs, ys)\n",
        "\n",
        "for j in range(len(alphas)):\n",
        "  alpha = alphas[j]\n",
        "  xmin = results[j]\n",
        "  vxmin = V(xmin, a, b, c, d)\n",
        "  plt.plot(xmin, vxmin, marker='.', linestyle=\"None\", label=str(alpha), color=colors[alpha], ms=10)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4G3kCAEJQ54"
      },
      "source": [
        "# Aufgabe 4 - Zusatz\r\n",
        "\r\n",
        "Im [Colab zu Vorlesung](https://colab.research.google.com/drive/1W4cLIHjk1uKgf2qXmegOPVqD0BzpvhLf?usp=sharing#scrollTo=hlnSECCdse_b) wird mittels Gradientenabstieg der funktionale Zusammenhang zwischen Wohnungsgröße und Preis ermittelt. Erhöht man die Zahl der Epochen (z.B. auf 100) und lässt den Code immer wieder laufen, biegt der Gradientenabstieg, in der Talsohle angekommen, mal nach rechts, mal nach links ab. Die gesamte Talsohle scheint optimal zu sein. Denken Sie darüber nach, wovon es tatsächlich abhängt, in welche Richtung abgebogen wird. Sie können auch den Code verändern, um ihre Gedanken zu überprüfen und die geschlossene Lösung im Code ergänzen.\r\n",
        "\r\n",
        "Hinweise:\r\n",
        "- Welche Bedeutung haben die beiden Parameter $w_0$ und $w_1$? **Achtung: Die Parameter $w_0$ und $w_1$ wurden im 3D-Plot im Colab versehentlich vertauscht, dieser Fehler hat auch seinen Weg in die Folien gefunden.**\r\n",
        "- Was ist zu erwarten, wenn man die Menge der Trainingsdaten, also die Zahl der zufällig generierten Wohnungs-Preis-Paare, sehr viel größer wählt?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg-6J-sJNBAP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}